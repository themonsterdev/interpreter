\hypertarget{class_tokenizer}{}\doxysection{Référence de la classe Tokenizer}
\label{class_tokenizer}\index{Tokenizer@{Tokenizer}}


{\ttfamily \#include $<$Tokenizer.\+h$>$}



Graphe de collaboration de Tokenizer\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=163pt]{class_tokenizer__coll__graph}
\end{center}
\end{figure}
\doxysubsection*{Fonctions membres publiques}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_tokenizer_a2052eaf316eec192c11b5f7688184bbf}{Tokenizer}} (string characters)
\item 
Token\+List \mbox{\hyperlink{class_tokenizer_ad4eb1a3fa978a7fbfa8027407cea945b}{Get\+Tokens}} ()
\end{DoxyCompactItemize}
\doxysubsection*{Fonctions membres publiques statiques}
\begin{DoxyCompactItemize}
\item 
static void \mbox{\hyperlink{class_tokenizer_a1c2ef0cc3fe72bdf1f186ae6fae8eaaf}{Print\+Tokens}} (Token\+List tokens)
\end{DoxyCompactItemize}


\doxysubsection{Description détaillée}
Classe \mbox{\hyperlink{class_tokenizer}{Tokenizer}}

Cette classe fais une analyse lexical sur une chaine de caractères données.

\begin{DoxySeeAlso}{Voir également}
\href{https://fr.wikipedia.org/wiki/Analyse_lexicale}{\texttt{ https\+://fr.\+wikipedia.\+org/wiki/\+Analyse\+\_\+lexicale}} 
\end{DoxySeeAlso}


\doxysubsection{Documentation des constructeurs et destructeur}
\mbox{\Hypertarget{class_tokenizer_a2052eaf316eec192c11b5f7688184bbf}\label{class_tokenizer_a2052eaf316eec192c11b5f7688184bbf}} 
\index{Tokenizer@{Tokenizer}!Tokenizer@{Tokenizer}}
\index{Tokenizer@{Tokenizer}!Tokenizer@{Tokenizer}}
\doxysubsubsection{\texorpdfstring{Tokenizer()}{Tokenizer()}}
{\footnotesize\ttfamily Tokenizer\+::\+Tokenizer (\begin{DoxyParamCaption}\item[{string}]{characters }\end{DoxyParamCaption})}

Constructeur \mbox{\hyperlink{class_tokenizer}{Tokenizer}}


\begin{DoxyParams}{Paramètres}
{\em characters} & les caractère d\textquotesingle{}entrée du program/script \\
\hline
\end{DoxyParams}


\doxysubsection{Documentation des fonctions membres}
\mbox{\Hypertarget{class_tokenizer_ad4eb1a3fa978a7fbfa8027407cea945b}\label{class_tokenizer_ad4eb1a3fa978a7fbfa8027407cea945b}} 
\index{Tokenizer@{Tokenizer}!GetTokens@{GetTokens}}
\index{GetTokens@{GetTokens}!Tokenizer@{Tokenizer}}
\doxysubsubsection{\texorpdfstring{GetTokens()}{GetTokens()}}
{\footnotesize\ttfamily Token\+List Tokenizer\+::\+Get\+Tokens (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

Récupèrer les tokens de la chaine de characters.

\begin{DoxyReturn}{Renvoie}
vector$<$\+Token$>$ les tokens générer 
\end{DoxyReturn}
\mbox{\Hypertarget{class_tokenizer_a1c2ef0cc3fe72bdf1f186ae6fae8eaaf}\label{class_tokenizer_a1c2ef0cc3fe72bdf1f186ae6fae8eaaf}} 
\index{Tokenizer@{Tokenizer}!PrintTokens@{PrintTokens}}
\index{PrintTokens@{PrintTokens}!Tokenizer@{Tokenizer}}
\doxysubsubsection{\texorpdfstring{PrintTokens()}{PrintTokens()}}
{\footnotesize\ttfamily static void Tokenizer\+::\+Print\+Tokens (\begin{DoxyParamCaption}\item[{Token\+List}]{tokens }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

Imprime les tokens générer

\begin{DoxyReturn}{Renvoie}
void 
\end{DoxyReturn}


La documentation de cette classe a été générée à partir du fichier suivant \+:\begin{DoxyCompactItemize}
\item 
lib/tokenizer/include/Tokenizer.\+h\end{DoxyCompactItemize}
